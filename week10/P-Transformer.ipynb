{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets\n",
    "python -m pip install --upgrade pip\n",
    "pip install torch\n",
    "pip install transformers\n",
    "\n",
    "pip install protobuf#???? 설치해야하나?\n",
    "pip install sentencepiece\n",
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src_input_ids': tensor([250004,  32964,  27150,      4,  22392,  11280,      7,    621,  50782,\n",
      "         43573,   5941,    373, 114942,      5,      2,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1]), 'tgt_input_ids': tensor([250004,  58320,  95350,  23739,     13,  63804,   1276,    566,  36443,\n",
      "            33,     23,    122,  42231,   6179,     56,  57891,   7560,      5,\n",
      "             2,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1,      1,      1,      1,      1,      1,      1,      1,\n",
      "             1,      1]), 'src_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'tgt_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50Tokenizer\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader, DistributedSampler\n",
    "\n",
    "\n",
    "#토크나이저\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer = MBart50Tokenizer.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.src_lang = 'en_XX'\n",
    "tokenizer.tgt_lang='de_DE'\n",
    "#데이터셋 로드\n",
    "ds = load_dataset(\"bentrevett/multi30k\")\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_len=128):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):#소스,타겟문장 추출\n",
    "        src_text = self.dataset[idx]['en']\n",
    "        tgt_text = self.dataset[idx]['de']\n",
    "\n",
    "        src_tokens = self.tokenizer(src_text, padding='max_length', truncation=True, max_length=self.max_len, return_tensors=\"pt\", add_special_tokens=True)\n",
    "        tgt_tokens = self.tokenizer(tgt_text, padding='max_length', truncation=True, max_length=self.max_len, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "        return {\n",
    "            'src_input_ids': src_tokens['input_ids'].squeeze(),\n",
    "            'tgt_input_ids': tgt_tokens['input_ids'].squeeze(),\n",
    "            'src_attention_mask': src_tokens['attention_mask'].squeeze(),\n",
    "            'tgt_attention_mask': tgt_tokens['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "\n",
    "#데이터셋-> 전처리된 데이터셋으로 변환\n",
    "train_dataset = TranslationDataset(ds['train'],tokenizer)\n",
    "val_dataset = TranslationDataset(ds['validation'],tokenizer)\n",
    "test_dataset = TranslationDataset(ds['test'],tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "\n",
    "#실제 토큰화된 결과 확인\n",
    "for batch in train_dataset:\n",
    "  print(batch)\n",
    "  break\n",
    "#<BOS> 250004 / <EOS> 2 / <PAD> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "from torch.nn import TransformerEncoderLayer, TransformerDecoderLayer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 단어 순서 개념(notion)을 토큰 임베딩에 도입하기 위한 위치 인코딩(positional encoding)을 위한 헬퍼 모듈(Module)\n",
    "class ParallelSeq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, num_layers: int, emb_size: int, nhead: int, src_vocab_size: int, tgt_vocab_size: int, dim_feedforward: int = 512, dropout: float = 0.1):\n",
    "        super(ParallelSeq2SeqTransformer, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.enc_layers = nn.ModuleList([TransformerEncoderLayer(emb_size, nhead, dim_feedforward, dropout) for _ in range(num_layers)])\n",
    "        self.dec_layers = nn.ModuleList([TransformerDecoderLayer(emb_size, nhead, dim_feedforward, dropout) for _ in range(num_layers)])\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout)\n",
    "\n",
    "    def forward(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "\n",
    "        # Parallel Encoder Processing\n",
    "        memory = src_emb\n",
    "        for layer in self.enc_layers:\n",
    "            memory = layer(memory, src_mask, src_padding_mask)\n",
    "\n",
    "        # Parallel Decoder Processing\n",
    "        output = tgt_emb\n",
    "        for layer in self.dec_layers:\n",
    "            output = layer(output, memory, tgt_mask, None, tgt_padding_mask, memory_key_padding_mask)\n",
    "\n",
    "        return self.generator(output)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: torch.Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "    \n",
    "# 입력 인덱스의 텐서를 해당하는 토큰 임베딩의 텐서로 변환하기 위한 헬퍼 모듈(Module)\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)#출력 생성기\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)#source 토큰 임베딩\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)#target 토큰 임베딩\n",
    "        self.positional_encoding = PositionalEncoding(#위치 인코딩\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    #인코더-디코더 각 과정/훈련데이터 처리\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        #source 문장을 처리한 후, target문장을 디코더로 처리?\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)#mask 적용\n",
    "        return self.generator(outs)#최종적으로 각 타겟 단어에 대한 확률분포 반환/임베딩 차원->선형변환 적용->확률분포\n",
    "\n",
    "    #추론(예측) 과정\n",
    "    #인코딩\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):#source 문장 인코딩\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "    #디코딩\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):#target 문장 디코딩\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#마스킹 생성\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)#상삼각행렬 생성(순차적인 마스킹 적용)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))#-무한대로 마스킹 적용\n",
    "    return mask\n",
    "\n",
    "#마스크 설정\n",
    "def create_mask(src, tgt):\n",
    "   #[0]: 배치크기, [1]:시퀀스 길이\n",
    "    tgt_seq_len = tgt.shape[0]-1#128-1\n",
    "\n",
    "\n",
    "    #어텐션 masking(디코더)\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len).type(torch.float32).to(DEVICE)\n",
    "\n",
    "    #패딩 토큰에 대한 masking(인코더/디코더)\n",
    "    src_padding_mask = (src == tokenizer.pad_token_id).to(torch.bool).transpose(0,1)#[src_seq_len, batch_size]\n",
    "    tgt_padding_mask = (tgt[:-1,:] == tokenizer.pad_token_id).to(torch.bool).transpose(0,1)\n",
    "\n",
    "    #차원 확인용\n",
    "    #print('tgt_seq_len',tgt_seq_len, '  tgt_mask',tgt_mask.shape  )\n",
    "    #print('\\n',src_padding_mask.shape, '\\n',tgt_padding_mask.shape)\n",
    "\n",
    "    return None, tgt_mask, src_padding_mask, tgt_padding_mask #src_mask는 반환하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = tokenizer.vocab_size\n",
    "TGT_VOCAB_SIZE = tokenizer.vocab_size\n",
    "EMB_SIZE = 512#512\n",
    "NHEAD = 4#8\n",
    "FFN_HID_DIM = 512#2048\n",
    "BATCH_SIZE = 8\n",
    "NUM_ENCODER_LAYERS = 6 #6\n",
    "NUM_DECODER_LAYERS = 6 #6\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "model = transformer.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLEU score 계산\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "# BLEU-4 점수 계산 함수\n",
    "def calculate_bleu(output_ids, target_ids, tokenizer):\n",
    "    total_bleu = 0\n",
    "    total_sentences = 0\n",
    "\n",
    "    for i in range(output_ids.size(0)):  # 배치의 각 문장에 대해 BLEU 계산\n",
    "        pred_sentence = tokenizer.decode(output_ids[i], skip_special_tokens=True)  # 예측된 문장\n",
    "        tgt_sentence = tokenizer.decode(target_ids[i], skip_special_tokens=True)  # 실제 문장\n",
    "\n",
    "        # BLEU-4 계산\n",
    "        reference = [tgt_sentence.split()]  # 참조 문장\n",
    "        candidate = pred_sentence.split()  # 예측 문장\n",
    "        bleu_score = sentence_bleu(reference, candidate, smoothing_function=smoothie)\n",
    "        total_bleu += bleu_score\n",
    "        total_sentences += 1\n",
    "\n",
    "    avg_bleu = total_bleu / total_sentences if total_sentences > 0 else 0\n",
    "    return avg_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "#손실 함수에 EOS 토큰 가중치 적용\n",
    "def create_weighted_loss(vocab_size, eos_token_id, pad_token_id, device):\n",
    "    weights = torch.ones(vocab_size).to(device)  #GPU로 옮김\n",
    "    weights[eos_token_id] = 0.1 #<EOS> 토큰에 대한 가중치를 낮춤\n",
    "    weights[pad_token_id] = 0.0#<PAD> 토큰은 무시\n",
    "\n",
    "    return nn.CrossEntropyLoss(weight=weights, ignore_index=pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/10:   4%|▍         | 161/3625 [00:24<08:42,  6.63batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 166\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# 훈련 - tqdm 적용\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m--> 166\u001b[0m     train_loss, train_bleu \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Bleu: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_bleu\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# 검증 - tqdm 적용\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 52\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, optimizer, dataloader, loss_fn, device, pbar)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m#BLEU-4 스코어 계산\u001b[39;00m\n\u001b[1;32m     51\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;66;03m#예측 토큰 -> [batch_size, sequence_length]\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m avg_bleu \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_bleu\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# BLEU 계산\u001b[39;00m\n\u001b[1;32m     53\u001b[0m total_bleu \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m avg_bleu\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#tqdm 업데이트\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36mcalculate_bleu\u001b[0;34m(output_ids, target_ids, tokenizer)\u001b[0m\n\u001b[1;32m      9\u001b[0m total_sentences \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(output_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)):  \u001b[38;5;66;03m# 배치의 각 문장에 대해 BLEU 계산\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     pred_sentence \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 예측된 문장\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     tgt_sentence \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(target_ids[i], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# 실제 문장\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# BLEU-4 계산\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4007\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   4004\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   4005\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 4007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4011\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4012\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py:1088\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode\u001b[39m(\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1080\u001b[0m     token_ids: Union[\u001b[38;5;28mint\u001b[39m, List[\u001b[38;5;28mint\u001b[39m]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1085\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_use_source_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_source_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1088\u001b[0m     filtered_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;66;03m# If given is a single id, prevents splitting the string in upcoming loop\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filtered_tokens, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py:1064\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[1;32m   1063\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(index)\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_ids\u001b[49m:\n\u001b[1;32m   1065\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_decoder:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1376\u001b[0m, in \u001b[0;36mSpecialTokensMixin.all_special_ids\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_special_ids\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;124;03m    `List[int]`: List the ids of the special tokens(`'<unk>'`, `'<cls>'`, etc.) mapped to class attributes.\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1376\u001b[0m     all_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_tokens\u001b[49m\n\u001b[1;32m   1377\u001b[0m     all_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(all_toks)\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_ids\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1368\u001b[0m, in \u001b[0;36mSpecialTokensMixin.all_special_tokens\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mall_special_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;124;03m    `List[str]`: A list of the unique special tokens (`'<unk>'`, `'<cls>'`, ..., etc.).\u001b[39;00m\n\u001b[1;32m   1365\u001b[0m \n\u001b[1;32m   1366\u001b[0m \u001b[38;5;124;03m    Convert tokens of `tokenizers.AddedToken` type to string.\u001b[39;00m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1368\u001b[0m     all_toks \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_tokens_extended\u001b[49m]\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_toks\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1354\u001b[0m, in \u001b[0;36mSpecialTokensMixin.all_special_tokens_extended\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_tokens_map_extended\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m   1353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m-> 1354\u001b[0m         tokens_to_add \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m value \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(token) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m seen]\n\u001b[1;32m   1355\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1356\u001b[0m         tokens_to_add \u001b[38;5;241m=\u001b[39m [value] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(value) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m seen \u001b[38;5;28;01melse\u001b[39;00m []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1354\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_tokens_map_extended\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m   1353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m-> 1354\u001b[0m         tokens_to_add \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m value \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m seen]\n\u001b[1;32m   1355\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1356\u001b[0m         tokens_to_add \u001b[38;5;241m=\u001b[39m [value] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(value) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m seen \u001b[38;5;28;01melse\u001b[39;00m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.cuda.amp as amp\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import time\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 손실 함수 및 옵티마이저\n",
    "#loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "loss_fn = create_weighted_loss(vocab_size=tokenizer.vocab_size, eos_token_id=eos_token_id, pad_token_id=pad_token_id, device=DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 학습 함수\n",
    "def train_epoch(model, optimizer, dataloader, loss_fn, device, pbar):\n",
    "    model.train()  # 모델을 학습 모드로 전환\n",
    "    total_loss = 0\n",
    "    total_bleu=0\n",
    "    scaler = amp.GradScaler()  # Mixed Precision에 필요한 GradScaler 초기화\n",
    "\n",
    "\n",
    "    for batch in dataloader:\n",
    "        #데이터 로드\n",
    "        src = batch['src_input_ids'].to(device).transpose(0,1)\n",
    "        tgt = batch['tgt_input_ids'].to(device).transpose(0,1)\n",
    "\n",
    "         #마스크 생성\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt)\n",
    "\n",
    "        #옵티마이저 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with amp.autocast():\n",
    "            #모델의 출력 계산\n",
    "            output = model(src, tgt[:-1, :], None, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "            #손실 계산 (output을 [batch_size, sequence_len, vocab_size]로 변환)\n",
    "            tgt_out = tgt[1:, :]  #<BOS> 토큰 제외\n",
    "            loss = loss_fn(output.reshape(-1, output.shape[-1]), tgt_out.reshape(-1))\n",
    "\n",
    "        # 역전파 및 파라미터 업데이트\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        #BLEU-4 스코어 계산\n",
    "        output_ids = output.argmax(dim=-1).transpose(0,1)#예측 토큰 -> [batch_size, sequence_length]\n",
    "        avg_bleu = calculate_bleu(output_ids, tgt[1:, :].transpose(0, 1), tokenizer)  # BLEU 계산\n",
    "        total_bleu += avg_bleu\n",
    "\n",
    "\n",
    "        #tqdm 업데이트\n",
    "        pbar.update(1)\n",
    "\n",
    "        #평균계산\n",
    "        avg_loss = total_loss/len(dataloader)\n",
    "        avg_bleu = total_bleu/len(dataloader)\n",
    "\n",
    "    return avg_loss, avg_bleu\n",
    "\n",
    "\n",
    "\n",
    "# 평가 함수\n",
    "def evaluate(model, dataloader, loss_fn, device, pbar):\n",
    "    model.eval()  # 평가 모드\n",
    "    total_loss = 0\n",
    "    total_bleu=0\n",
    "\n",
    "    with torch.no_grad():  # 평가 시에는 역전파가 필요 없으므로 no_grad 사용\n",
    "        for batch in dataloader:\n",
    "            src = batch['src_input_ids'].to(device).transpose(0,1)\n",
    "            tgt = batch['tgt_input_ids'].to(device).transpose(0,1)\n",
    "\n",
    "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt)\n",
    "\n",
    "            output = model(src, tgt[:-1, :], None, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "            tgt_out = tgt[1:, :]\n",
    "            loss = loss_fn(output.reshape(-1, output.shape[-1]), tgt_out.reshape(-1))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            #BLEU-4 스코어 계산\n",
    "            output_ids = output.argmax(dim=-1).transpose(0,1)#예측 토큰 -> [batch_size, sequence_length]\n",
    "            avg_bleu = calculate_bleu(output_ids, tgt[1:, :].transpose(0, 1), tokenizer)  # BLEU 계산\n",
    "            total_bleu += avg_bleu\n",
    "\n",
    "\n",
    "            #tqdm 업데이트\n",
    "            pbar.update(1)\n",
    "\n",
    "            #평균계산\n",
    "            avg_loss = total_loss/len(dataloader)\n",
    "            avg_bleu = total_bleu/len(dataloader)\n",
    "\n",
    "\n",
    "    return avg_loss, avg_bleu\n",
    "\n",
    "\n",
    "#test\n",
    "def test_model(model, dataloader, loss_fn, device, tokenizer):\n",
    "    model.eval()\n",
    "    total_loss = 0  # 손실 누적\n",
    "    total_bleu = 0  # BLEU-4 스코어 누적\n",
    "    total_sentences = 0  # 문장 수 누적\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            src = batch['src_input_ids'].to(device).transpose(0, 1)\n",
    "            tgt = batch['tgt_input_ids'].to(device).transpose(0, 1)\n",
    "\n",
    "            # 마스크 생성\n",
    "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt)\n",
    "\n",
    "            with amp.autocast():\n",
    "                # 모델의 출력 계산\n",
    "                output = model(src, tgt[:-1, :], None, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "                # 손실 계산\n",
    "                tgt_out = tgt[1:, :]\n",
    "                loss = loss_fn(output.reshape(-1, output.shape[-1]), tgt_out.reshape(-1))\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            # BLEU-4 계산\n",
    "            output_ids = output.argmax(dim=-1).transpose(0, 1)  # 예측 토큰\n",
    "            avg_bleu = calculate_bleu(output_ids, tgt[1:, :].transpose(0, 1), tokenizer)  # BLEU 계산\n",
    "            total_bleu += avg_bleu\n",
    "            total_sentences += 1\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)  # 평균 손실\n",
    "    avg_bleu_score = total_bleu / total_sentences if total_sentences > 0 else 0  # 평균 BLEU-4 스코어\n",
    "\n",
    "    return avg_loss, avg_bleu_score\n",
    "\n",
    "\n",
    "###################\n",
    "\n",
    "train_dataset = TranslationDataset(ds['train'],tokenizer)\n",
    "val_dataset = TranslationDataset(ds['validation'],tokenizer)\n",
    "test_dataset = TranslationDataset(ds['test'],tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "\"\"\"\n",
    "small_train_dataset = TranslationDataset(ds['train'].select(range(200)),tokenizer)\n",
    "small_val_dataset = TranslationDataset(ds['validation'].select(range(200)),tokenizer)\n",
    "small_test_dataset =  TranslationDataset(ds['test'].select(range(200)),tokenizer)\n",
    "train_dataloader = DataLoader(small_train_dataset, batch_size=8)\n",
    "val_dataloader = DataLoader(small_val_dataset, batch_size=8)\n",
    "test_dataloader = DataLoader(small_test_dataset, batch_size=8) \"\"\"\n",
    "\n",
    "\n",
    "# 에폭 반복문\n",
    "EPOCHS = 5  # 원하는 에폭 수 설정\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "\n",
    "    # 훈련 - tqdm 적용\n",
    "    with tqdm(total=len(train_dataloader), desc=f\"Training Epoch {epoch}/{EPOCHS}\", unit=\"batch\") as pbar:\n",
    "        train_loss, train_bleu = train_epoch(model, optimizer, train_dataloader, loss_fn, DEVICE, pbar)\n",
    "    print(f\"  Train loss: {train_loss}, Train Bleu: {train_bleu}\")\n",
    "\n",
    "    # 검증 - tqdm 적용\n",
    "    with tqdm(total=len(val_dataloader), desc=f\"Validation Epoch {epoch}/{EPOCHS}\", unit=\"batch\") as pbar:\n",
    "        val_loss, val_bleu = evaluate(model, val_dataloader, loss_fn, DEVICE, pbar)\n",
    "    print(f\"  Validation loss: {val_loss}, Validation Bleu: {val_bleu}\\n\")\n",
    "\n",
    "# 학습이 끝난 후 테스트 데이터로 평가\n",
    "print(\"\\n\\nEvaluating on Test dataset : \")\n",
    "test_loss, test_bleu = test_model(model, test_dataloader, loss_fn, DEVICE, tokenizer)\n",
    "print(f\"Test loss: {test_loss}, Test BLEU-4: {test_bleu}\")\n",
    "\n",
    "# 학습 종료 시간\n",
    "end_time = time.time()\n",
    "\n",
    "# 총 소요 시간 계산\n",
    "total_time = end_time - start_time\n",
    "print(f'Total Training Time: {total_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "from torch.nn import TransformerEncoderLayer, TransformerDecoderLayer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 단어 순서 개념(notion)을 토큰 임베딩에 도입하기 위한 위치 인코딩(positional encoding)을 위한 헬퍼 모듈(Module)\n",
    "class ParallelTransformer(nn.Module):\n",
    "    def __init__(self, num_layers: int, emb_size: int, nhead: int, src_vocab_size: int, tgt_vocab_size: int, dim_feedforward: int = 1024, dropout: float = 0.1):\n",
    "        super(ParallelTransformer, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        #인코더->디코더 변환 레이어\n",
    "        self.enc_to_dec_proj = nn.Linear(emb_size, emb_size)\n",
    "        \n",
    "        self.enc_layers = nn.ModuleList([TransformerEncoderLayer(emb_size, nhead, dim_feedforward, dropout) for _ in range(num_layers)])\n",
    "        self.dec_layers = nn.ModuleList([TransformerDecoderLayer(emb_size, nhead, dim_feedforward, dropout) for _ in range(num_layers)])\n",
    "        \n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)#출력레이어\n",
    "        \n",
    "        #임베딩 레이어\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout)\n",
    "\n",
    "    def forward(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "\n",
    "        memory = src_emb\n",
    "        output = tgt_emb\n",
    "\n",
    "        #인코더->디코더 순으로 레이어들을 순차적으로 통과\n",
    "        for i in range(self.num_layers):\n",
    "            #인코더\n",
    "            memory = self.enc_layers[i](memory, src_mask, src_padding_mask)\n",
    "            \n",
    "            #디코더\n",
    "            output = self.dec_layers[i](output, memory, tgt_mask, None, tgt_padding_mask, memory_key_padding_mask)\n",
    "        \n",
    "        return self.generator(output)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: torch.Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "    \n",
    "# 입력 인덱스의 텐서를 해당하는 토큰 임베딩의 텐서로 변환하기 위한 헬퍼 모듈(Module)\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)#출력 생성기\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)#source 토큰 임베딩\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)#target 토큰 임베딩\n",
    "        self.positional_encoding = PositionalEncoding(#위치 인코딩\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    #인코더-디코더 각 과정/훈련데이터 처리\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        #source 문장을 처리한 후, target문장을 디코더로 처리?\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)#mask 적용\n",
    "        return self.generator(outs)#최종적으로 각 타겟 단어에 대한 확률분포 반환/임베딩 차원->선형변환 적용->확률분포\n",
    "\n",
    "    #추론(예측) 과정\n",
    "    #인코딩\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):#source 문장 인코딩\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "    #디코딩\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):#target 문장 디코딩\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#마스킹 생성\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)#상삼각행렬 생성(순차적인 마스킹 적용)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))#-무한대로 마스킹 적용\n",
    "    return mask\n",
    "\n",
    "#마스크 설정\n",
    "def create_mask(src, tgt):\n",
    "   #[0]: 배치크기, [1]:시퀀스 길이\n",
    "    tgt_seq_len = tgt.shape[0]-1#128-1\n",
    "\n",
    "\n",
    "    #어텐션 masking(디코더)\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len).type(torch.float32).to(DEVICE)\n",
    "\n",
    "    #패딩 토큰에 대한 masking(인코더/디코더)\n",
    "    src_padding_mask = (src == tokenizer.pad_token_id).to(torch.bool).transpose(0,1)#[src_seq_len, batch_size]\n",
    "    tgt_padding_mask = (tgt[:-1,:] == tokenizer.pad_token_id).to(torch.bool).transpose(0,1)\n",
    "\n",
    "    #차원 확인용\n",
    "    #print('tgt_seq_len',tgt_seq_len, '  tgt_mask',tgt_mask.shape  )\n",
    "    #print('\\n',src_padding_mask.shape, '\\n',tgt_padding_mask.shape)\n",
    "\n",
    "    return None, tgt_mask, src_padding_mask, tgt_padding_mask #src_mask는 반환하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = tokenizer.vocab_size\n",
    "TGT_VOCAB_SIZE = tokenizer.vocab_size\n",
    "EMB_SIZE = 512#512\n",
    "NHEAD = 4#8\n",
    "FFN_HID_DIM = 512#2048\n",
    "BATCH_SIZE = 8\n",
    "NUM_ENCODER_LAYERS = 6 #6\n",
    "NUM_DECODER_LAYERS = 6 #6\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "model = transformer.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLEU score 계산\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "# BLEU-4 점수 계산 함수\n",
    "def calculate_bleu(output_ids, target_ids, tokenizer):\n",
    "    total_bleu = 0\n",
    "    total_sentences = 0\n",
    "\n",
    "    for i in range(output_ids.size(0)):  # 배치의 각 문장에 대해 BLEU 계산\n",
    "        pred_sentence = tokenizer.decode(output_ids[i], skip_special_tokens=True)  # 예측된 문장\n",
    "        tgt_sentence = tokenizer.decode(target_ids[i], skip_special_tokens=True)  # 실제 문장\n",
    "\n",
    "        # BLEU-4 계산\n",
    "        reference = [tgt_sentence.split()]  # 참조 문장\n",
    "        candidate = pred_sentence.split()  # 예측 문장\n",
    "        bleu_score = sentence_bleu(reference, candidate, smoothing_function=smoothie)\n",
    "        total_bleu += bleu_score\n",
    "        total_sentences += 1\n",
    "\n",
    "    avg_bleu = total_bleu / total_sentences if total_sentences > 0 else 0\n",
    "    return avg_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "#손실 함수에 EOS 토큰 가중치 적용\n",
    "def create_weighted_loss(vocab_size, eos_token_id, pad_token_id, device):\n",
    "    weights = torch.ones(vocab_size).to(device)  #GPU로 옮김\n",
    "    weights[eos_token_id] = 0.1 #<EOS> 토큰에 대한 가중치를 낮춤\n",
    "    weights[pad_token_id] = 0.0#<PAD> 토큰은 무시\n",
    "\n",
    "    return nn.CrossEntropyLoss(weight=weights, ignore_index=pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/5:   0%|          | 0/3625 [00:00<?, ?batch/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5076: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/5: 100%|██████████| 3625/3625 [09:04<00:00,  6.66batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train loss: 4.963761483948806, Train Bleu: 0.049792581369893715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 1/5: 100%|██████████| 127/127 [00:10<00:00, 12.53batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation loss: 3.904220746258112, Validation Bleu: 0.07591318384091325\n",
      "\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/5: 100%|██████████| 3625/3625 [09:02<00:00,  6.68batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train loss: 3.554837892071954, Train Bleu: 0.0763299039632607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 2/5: 100%|██████████| 127/127 [00:10<00:00, 12.68batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation loss: 3.242792005614033, Validation Bleu: 0.10317179598069949\n",
      "\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/5: 100%|██████████| 3625/3625 [09:01<00:00,  6.69batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train loss: 3.0355732363010275, Train Bleu: 0.09102686387542122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 3/5: 100%|██████████| 127/127 [00:10<00:00, 12.70batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation loss: 2.915244609352172, Validation Bleu: 0.1127169018317616\n",
      "\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4/5: 100%|██████████| 3625/3625 [09:02<00:00,  6.69batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train loss: 2.7084292000737684, Train Bleu: 0.09993656659592934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 4/5: 100%|██████████| 127/127 [00:10<00:00, 12.61batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation loss: 2.700872539535282, Validation Bleu: 0.12045635649431972\n",
      "\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5/5: 100%|██████████| 3625/3625 [09:02<00:00,  6.69batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train loss: 2.46573330221505, Train Bleu: 0.10726020946304046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 5/5: 100%|██████████| 127/127 [00:10<00:00, 12.62batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation loss: 2.58388367224866, Validation Bleu: 0.12465180463960317\n",
      "\n",
      "\n",
      "\n",
      "Evaluating on Test dataset : \n",
      "Test loss: 2.5243550882339476, Test BLEU-4: 0.13146173478009524\n",
      "Total Training Time: 2771.30 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch.cuda.amp as amp\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import time\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 손실 함수 및 옵티마이저\n",
    "#loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "loss_fn = create_weighted_loss(vocab_size=tokenizer.vocab_size, eos_token_id=eos_token_id, pad_token_id=pad_token_id, device=DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 학습 함수\n",
    "def train_epoch(model, optimizer, dataloader, loss_fn, device, pbar):\n",
    "    model.train()  # 모델을 학습 모드로 전환\n",
    "    total_loss = 0\n",
    "    total_bleu=0\n",
    "    scaler = amp.GradScaler()  # Mixed Precision에 필요한 GradScaler 초기화\n",
    "\n",
    "\n",
    "    for batch in dataloader:\n",
    "        #데이터 로드\n",
    "        src = batch['src_input_ids'].to(device).transpose(0,1)\n",
    "        tgt = batch['tgt_input_ids'].to(device).transpose(0,1)\n",
    "\n",
    "         #마스크 생성\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt)\n",
    "\n",
    "        #옵티마이저 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with amp.autocast():\n",
    "            #모델의 출력 계산\n",
    "            output = model(src, tgt[:-1, :], None, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "            #손실 계산 (output을 [batch_size, sequence_len, vocab_size]로 변환)\n",
    "            tgt_out = tgt[1:, :]  #<BOS> 토큰 제외\n",
    "            loss = loss_fn(output.reshape(-1, output.shape[-1]), tgt_out.reshape(-1))\n",
    "\n",
    "        # 역전파 및 파라미터 업데이트\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        #BLEU-4 스코어 계산\n",
    "        output_ids = output.argmax(dim=-1).transpose(0,1)#예측 토큰 -> [batch_size, sequence_length]\n",
    "        avg_bleu = calculate_bleu(output_ids, tgt[1:, :].transpose(0, 1), tokenizer)  # BLEU 계산\n",
    "        total_bleu += avg_bleu\n",
    "\n",
    "\n",
    "        #tqdm 업데이트\n",
    "        pbar.update(1)\n",
    "\n",
    "        #평균계산\n",
    "        avg_loss = total_loss/len(dataloader)\n",
    "        avg_bleu = total_bleu/len(dataloader)\n",
    "\n",
    "    return avg_loss, avg_bleu\n",
    "\n",
    "\n",
    "\n",
    "# 평가 함수\n",
    "def evaluate(model, dataloader, loss_fn, device, pbar):\n",
    "    model.eval()  # 평가 모드\n",
    "    total_loss = 0\n",
    "    total_bleu=0\n",
    "\n",
    "    with torch.no_grad():  # 평가 시에는 역전파가 필요 없으므로 no_grad 사용\n",
    "        for batch in dataloader:\n",
    "            src = batch['src_input_ids'].to(device).transpose(0,1)\n",
    "            tgt = batch['tgt_input_ids'].to(device).transpose(0,1)\n",
    "\n",
    "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt)\n",
    "\n",
    "            output = model(src, tgt[:-1, :], None, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "            tgt_out = tgt[1:, :]\n",
    "            loss = loss_fn(output.reshape(-1, output.shape[-1]), tgt_out.reshape(-1))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            #BLEU-4 스코어 계산\n",
    "            output_ids = output.argmax(dim=-1).transpose(0,1)#예측 토큰 -> [batch_size, sequence_length]\n",
    "            avg_bleu = calculate_bleu(output_ids, tgt[1:, :].transpose(0, 1), tokenizer)  # BLEU 계산\n",
    "            total_bleu += avg_bleu\n",
    "\n",
    "\n",
    "            #tqdm 업데이트\n",
    "            pbar.update(1)\n",
    "\n",
    "            #평균계산\n",
    "            avg_loss = total_loss/len(dataloader)\n",
    "            avg_bleu = total_bleu/len(dataloader)\n",
    "\n",
    "\n",
    "    return avg_loss, avg_bleu\n",
    "\n",
    "\n",
    "#test\n",
    "def test_model(model, dataloader, loss_fn, device, tokenizer):\n",
    "    model.eval()\n",
    "    total_loss = 0  # 손실 누적\n",
    "    total_bleu = 0  # BLEU-4 스코어 누적\n",
    "    total_sentences = 0  # 문장 수 누적\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            src = batch['src_input_ids'].to(device).transpose(0, 1)\n",
    "            tgt = batch['tgt_input_ids'].to(device).transpose(0, 1)\n",
    "\n",
    "            # 마스크 생성\n",
    "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt)\n",
    "\n",
    "            with amp.autocast():\n",
    "                # 모델의 출력 계산\n",
    "                output = model(src, tgt[:-1, :], None, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "                # 손실 계산\n",
    "                tgt_out = tgt[1:, :]\n",
    "                loss = loss_fn(output.reshape(-1, output.shape[-1]), tgt_out.reshape(-1))\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            # BLEU-4 계산\n",
    "            output_ids = output.argmax(dim=-1).transpose(0, 1)  # 예측 토큰\n",
    "            avg_bleu = calculate_bleu(output_ids, tgt[1:, :].transpose(0, 1), tokenizer)  # BLEU 계산\n",
    "            total_bleu += avg_bleu\n",
    "            total_sentences += 1\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)  # 평균 손실\n",
    "    avg_bleu_score = total_bleu / total_sentences if total_sentences > 0 else 0  # 평균 BLEU-4 스코어\n",
    "\n",
    "    return avg_loss, avg_bleu_score\n",
    "\n",
    "\n",
    "###################\n",
    "\n",
    "train_dataset = TranslationDataset(ds['train'],tokenizer)\n",
    "val_dataset = TranslationDataset(ds['validation'],tokenizer)\n",
    "test_dataset = TranslationDataset(ds['test'],tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "\"\"\"\n",
    "small_train_dataset = TranslationDataset(ds['train'].select(range(200)),tokenizer)\n",
    "small_val_dataset = TranslationDataset(ds['validation'].select(range(200)),tokenizer)\n",
    "small_test_dataset =  TranslationDataset(ds['test'].select(range(200)),tokenizer)\n",
    "train_dataloader = DataLoader(small_train_dataset, batch_size=8)\n",
    "val_dataloader = DataLoader(small_val_dataset, batch_size=8)\n",
    "test_dataloader = DataLoader(small_test_dataset, batch_size=8) \"\"\"\n",
    "\n",
    "\n",
    "# 에폭 반복문\n",
    "EPOCHS = 5  # 원하는 에폭 수 설정\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "\n",
    "    # 훈련 - tqdm 적용\n",
    "    with tqdm(total=len(train_dataloader), desc=f\"Training Epoch {epoch}/{EPOCHS}\", unit=\"batch\") as pbar:\n",
    "        train_loss, train_bleu = train_epoch(model, optimizer, train_dataloader, loss_fn, DEVICE, pbar)\n",
    "    print(f\"  Train loss: {train_loss}, Train Bleu: {train_bleu}\")\n",
    "\n",
    "    # 검증 - tqdm 적용\n",
    "    with tqdm(total=len(val_dataloader), desc=f\"Validation Epoch {epoch}/{EPOCHS}\", unit=\"batch\") as pbar:\n",
    "        val_loss, val_bleu = evaluate(model, val_dataloader, loss_fn, DEVICE, pbar)\n",
    "    print(f\"  Validation loss: {val_loss}, Validation Bleu: {val_bleu}\\n\")\n",
    "\n",
    "# 학습이 끝난 후 테스트 데이터로 평가\n",
    "print(\"\\n\\nEvaluating on Test dataset : \")\n",
    "test_loss, test_bleu = test_model(model, test_dataloader, loss_fn, DEVICE, tokenizer)\n",
    "print(f\"Test loss: {test_loss}, Test BLEU-4: {test_bleu}\")\n",
    "\n",
    "# 학습 종료 시간\n",
    "end_time = time.time()\n",
    "\n",
    "# 총 소요 시간 계산\n",
    "total_time = end_time - start_time\n",
    "print(f'Total Training Time: {total_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P-Transformer with Teacher Forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "from torch.nn import TransformerEncoderLayer, TransformerDecoderLayer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 단어 순서 개념(notion)을 토큰 임베딩에 도입하기 위한 위치 인코딩(positional encoding)을 위한 헬퍼 모듈(Module)\n",
    "class ParallelTransformer(nn.Module):\n",
    "    def __init__(self, num_layers: int, emb_size: int, nhead: int, src_vocab_size: int, tgt_vocab_size: int, dim_feedforward: int = 1024, dropout: float = 0.1):\n",
    "        super(ParallelTransformer, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        #인코더->디코더 변환 레이어\n",
    "        self.enc_to_dec_proj = nn.Linear(emb_size, emb_size)\n",
    "        \n",
    "        self.enc_layers = nn.ModuleList([TransformerEncoderLayer(emb_size, nhead, dim_feedforward, dropout) for _ in range(num_layers)])\n",
    "        self.dec_layers = nn.ModuleList([TransformerDecoderLayer(emb_size, nhead, dim_feedforward, dropout) for _ in range(num_layers)])\n",
    "        \n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)#출력레이어\n",
    "        \n",
    "        #임베딩 레이어\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout)\n",
    "\n",
    "    def forward(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask, teacher_forcing_ratio=0.5):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "\n",
    "        memory = src_emb\n",
    "        output = tgt_emb\n",
    "\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            memory = self.enc_layers[i](memory, src_mask, src_padding_mask)\n",
    "\n",
    "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            #teacher forcing\n",
    "            output = self.dec_layers[i](tgt_emb, memory, tgt_mask, None, tgt_padding_mask, memory_key_padding_mask)\n",
    "\n",
    "        return self.generator(output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: torch.Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "    \n",
    "# 입력 인덱스의 텐서를 해당하는 토큰 임베딩의 텐서로 변환하기 위한 헬퍼 모듈(Module)\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)#출력 생성기\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)#source 토큰 임베딩\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)#target 토큰 임베딩\n",
    "        self.positional_encoding = PositionalEncoding(#위치 인코딩\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    #인코더-디코더 각 과정/훈련데이터 처리\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        #source 문장을 처리한 후, target문장을 디코더로 처리?\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)#mask 적용\n",
    "        return self.generator(outs)#최종적으로 각 타겟 단어에 대한 확률분포 반환/임베딩 차원->선형변환 적용->확률분포\n",
    "\n",
    "    #추론(예측) 과정\n",
    "    #인코딩\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):#source 문장 인코딩\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "    #디코딩\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):#target 문장 디코딩\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#마스킹 생성\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)#상삼각행렬 생성(순차적인 마스킹 적용)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))#-무한대로 마스킹 적용\n",
    "    return mask\n",
    "\n",
    "#마스크 설정\n",
    "def create_mask(src, tgt):\n",
    "   #[0]: 배치크기, [1]:시퀀스 길이\n",
    "    tgt_seq_len = tgt.shape[0]-1#128-1\n",
    "\n",
    "\n",
    "    #어텐션 masking(디코더)\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len).type(torch.float32).to(DEVICE)\n",
    "\n",
    "    #패딩 토큰에 대한 masking(인코더/디코더)\n",
    "    src_padding_mask = (src == tokenizer.pad_token_id).to(torch.bool).transpose(0,1)#[src_seq_len, batch_size]\n",
    "    tgt_padding_mask = (tgt[:-1,:] == tokenizer.pad_token_id).to(torch.bool).transpose(0,1)\n",
    "\n",
    "    #차원 확인용\n",
    "    #print('tgt_seq_len',tgt_seq_len, '  tgt_mask',tgt_mask.shape  )\n",
    "    #print('\\n',src_padding_mask.shape, '\\n',tgt_padding_mask.shape)\n",
    "\n",
    "    return None, tgt_mask, src_padding_mask, tgt_padding_mask #src_mask는 반환하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = tokenizer.vocab_size\n",
    "TGT_VOCAB_SIZE = tokenizer.vocab_size\n",
    "EMB_SIZE = 512#512\n",
    "NHEAD = 4#8\n",
    "FFN_HID_DIM = 512#2048\n",
    "BATCH_SIZE = 8\n",
    "NUM_ENCODER_LAYERS = 6 #6\n",
    "NUM_DECODER_LAYERS = 6 #6\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "model = transformer.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLEU score 계산\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "# BLEU-4 점수 계산 함수\n",
    "def calculate_bleu(output_ids, target_ids, tokenizer):\n",
    "    total_bleu = 0\n",
    "    total_sentences = 0\n",
    "\n",
    "    for i in range(output_ids.size(0)):  # 배치의 각 문장에 대해 BLEU 계산\n",
    "        pred_sentence = tokenizer.decode(output_ids[i], skip_special_tokens=True)  # 예측된 문장\n",
    "        tgt_sentence = tokenizer.decode(target_ids[i], skip_special_tokens=True)  # 실제 문장\n",
    "\n",
    "        # BLEU-4 계산\n",
    "        reference = [tgt_sentence.split()]  # 참조 문장\n",
    "        candidate = pred_sentence.split()  # 예측 문장\n",
    "        bleu_score = sentence_bleu(reference, candidate, smoothing_function=smoothie)\n",
    "        total_bleu += bleu_score\n",
    "        total_sentences += 1\n",
    "\n",
    "    avg_bleu = total_bleu / total_sentences if total_sentences > 0 else 0\n",
    "    return avg_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "#손실 함수에 EOS 토큰 가중치 적용\n",
    "def create_weighted_loss(vocab_size, eos_token_id, pad_token_id, device):\n",
    "    weights = torch.ones(vocab_size).to(device)  #GPU로 옮김\n",
    "    weights[eos_token_id] = 0.1 #<EOS> 토큰에 대한 가중치를 낮춤\n",
    "    weights[pad_token_id] = 0.0#<PAD> 토큰은 무시\n",
    "\n",
    "    return nn.CrossEntropyLoss(weight=weights, ignore_index=pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.cuda.amp as amp\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import time\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 손실 함수 및 옵티마이저\n",
    "#loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "eos_token_id = tokenizer.eos_token_id\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "loss_fn = create_weighted_loss(vocab_size=tokenizer.vocab_size, eos_token_id=eos_token_id, pad_token_id=pad_token_id, device=DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# 학습 함수\n",
    "def train_epoch(model, optimizer, dataloader, loss_fn, device, pbar):\n",
    "    model.train()  # 모델을 학습 모드로 전환\n",
    "    total_loss = 0\n",
    "    total_bleu=0\n",
    "    scaler = amp.GradScaler()  # Mixed Precision에 필요한 GradScaler 초기화\n",
    "\n",
    "\n",
    "    for batch in dataloader:\n",
    "        #데이터 로드\n",
    "        src = batch['src_input_ids'].to(device).transpose(0,1)\n",
    "        tgt = batch['tgt_input_ids'].to(device).transpose(0,1)\n",
    "\n",
    "         #마스크 생성\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt)\n",
    "\n",
    "        #옵티마이저 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with amp.autocast():\n",
    "            #모델의 출력 계산\n",
    "            output = model(src, tgt[:-1, :], None, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "            #손실 계산 (output을 [batch_size, sequence_len, vocab_size]로 변환)\n",
    "            tgt_out = tgt[1:, :]  #<BOS> 토큰 제외\n",
    "            loss = loss_fn(output.reshape(-1, output.shape[-1]), tgt_out.reshape(-1))\n",
    "\n",
    "        # 역전파 및 파라미터 업데이트\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        #BLEU-4 스코어 계산\n",
    "        output_ids = output.argmax(dim=-1).transpose(0,1)#예측 토큰 -> [batch_size, sequence_length]\n",
    "        avg_bleu = calculate_bleu(output_ids, tgt[1:, :].transpose(0, 1), tokenizer)  # BLEU 계산\n",
    "        total_bleu += avg_bleu\n",
    "\n",
    "\n",
    "        #tqdm 업데이트\n",
    "        pbar.update(1)\n",
    "\n",
    "        #평균계산\n",
    "        avg_loss = total_loss/len(dataloader)\n",
    "        avg_bleu = total_bleu/len(dataloader)\n",
    "\n",
    "    return avg_loss, avg_bleu\n",
    "\n",
    "\n",
    "\n",
    "# 평가 함수\n",
    "def evaluate(model, dataloader, loss_fn, device, pbar):\n",
    "    model.eval()  # 평가 모드\n",
    "    total_loss = 0\n",
    "    total_bleu=0\n",
    "\n",
    "    with torch.no_grad():  # 평가 시에는 역전파가 필요 없으므로 no_grad 사용\n",
    "        for batch in dataloader:\n",
    "            src = batch['src_input_ids'].to(device).transpose(0,1)\n",
    "            tgt = batch['tgt_input_ids'].to(device).transpose(0,1)\n",
    "\n",
    "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt)\n",
    "\n",
    "            output = model(src, tgt[:-1, :], None, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "            tgt_out = tgt[1:, :]\n",
    "            loss = loss_fn(output.reshape(-1, output.shape[-1]), tgt_out.reshape(-1))\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            #BLEU-4 스코어 계산\n",
    "            output_ids = output.argmax(dim=-1).transpose(0,1)#예측 토큰 -> [batch_size, sequence_length]\n",
    "            avg_bleu = calculate_bleu(output_ids, tgt[1:, :].transpose(0, 1), tokenizer)  # BLEU 계산\n",
    "            total_bleu += avg_bleu\n",
    "\n",
    "\n",
    "            #tqdm 업데이트\n",
    "            pbar.update(1)\n",
    "\n",
    "            #평균계산\n",
    "            avg_loss = total_loss/len(dataloader)\n",
    "            avg_bleu = total_bleu/len(dataloader)\n",
    "\n",
    "\n",
    "    return avg_loss, avg_bleu\n",
    "\n",
    "\n",
    "#test\n",
    "def test_model(model, dataloader, loss_fn, device, tokenizer):\n",
    "    model.eval()\n",
    "    total_loss = 0  # 손실 누적\n",
    "    total_bleu = 0  # BLEU-4 스코어 누적\n",
    "    total_sentences = 0  # 문장 수 누적\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            src = batch['src_input_ids'].to(device).transpose(0, 1)\n",
    "            tgt = batch['tgt_input_ids'].to(device).transpose(0, 1)\n",
    "\n",
    "            # 마스크 생성\n",
    "            src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt)\n",
    "\n",
    "            with amp.autocast():\n",
    "                # 모델의 출력 계산\n",
    "                output = model(src, tgt[:-1, :], None, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "                # 손실 계산\n",
    "                tgt_out = tgt[1:, :]\n",
    "                loss = loss_fn(output.reshape(-1, output.shape[-1]), tgt_out.reshape(-1))\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            # BLEU-4 계산\n",
    "            output_ids = output.argmax(dim=-1).transpose(0, 1)  # 예측 토큰\n",
    "            avg_bleu = calculate_bleu(output_ids, tgt[1:, :].transpose(0, 1), tokenizer)  # BLEU 계산\n",
    "            total_bleu += avg_bleu\n",
    "            total_sentences += 1\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)  # 평균 손실\n",
    "    avg_bleu_score = total_bleu / total_sentences if total_sentences > 0 else 0  # 평균 BLEU-4 스코어\n",
    "\n",
    "    return avg_loss, avg_bleu_score\n",
    "\n",
    "\n",
    "###################\n",
    "\n",
    "train_dataset = TranslationDataset(ds['train'],tokenizer)\n",
    "val_dataset = TranslationDataset(ds['validation'],tokenizer)\n",
    "test_dataset = TranslationDataset(ds['test'],tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "\"\"\"\n",
    "small_train_dataset = TranslationDataset(ds['train'].select(range(200)),tokenizer)\n",
    "small_val_dataset = TranslationDataset(ds['validation'].select(range(200)),tokenizer)\n",
    "small_test_dataset =  TranslationDataset(ds['test'].select(range(200)),tokenizer)\n",
    "train_dataloader = DataLoader(small_train_dataset, batch_size=8)\n",
    "val_dataloader = DataLoader(small_val_dataset, batch_size=8)\n",
    "test_dataloader = DataLoader(small_test_dataset, batch_size=8) \"\"\"\n",
    "\n",
    "\n",
    "# 에폭 반복문\n",
    "EPOCHS = 5  # 원하는 에폭 수 설정\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "\n",
    "    # 훈련 - tqdm 적용\n",
    "    with tqdm(total=len(train_dataloader), desc=f\"Training Epoch {epoch}/{EPOCHS}\", unit=\"batch\") as pbar:\n",
    "        train_loss, train_bleu = train_epoch(model, optimizer, train_dataloader, loss_fn, DEVICE, pbar)\n",
    "    print(f\"  Train loss: {train_loss}, Train Bleu: {train_bleu}\")\n",
    "\n",
    "    # 검증 - tqdm 적용\n",
    "    with tqdm(total=len(val_dataloader), desc=f\"Validation Epoch {epoch}/{EPOCHS}\", unit=\"batch\") as pbar:\n",
    "        val_loss, val_bleu = evaluate(model, val_dataloader, loss_fn, DEVICE, pbar)\n",
    "    print(f\"  Validation loss: {val_loss}, Validation Bleu: {val_bleu}\\n\")\n",
    "\n",
    "# 학습이 끝난 후 테스트 데이터로 평가\n",
    "print(\"\\n\\nEvaluating on Test dataset : \")\n",
    "test_loss, test_bleu = test_model(model, test_dataloader, loss_fn, DEVICE, tokenizer)\n",
    "print(f\"Test loss: {test_loss}, Test BLEU-4: {test_bleu}\")\n",
    "\n",
    "# 학습 종료 시간\n",
    "end_time = time.time()\n",
    "\n",
    "# 총 소요 시간 계산\n",
    "total_time = end_time - start_time\n",
    "print(f'Total Training Time: {total_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "#번역 작업 수행\n",
    "def translate(model, sentence, tokenizer, device, max_length=128):\n",
    "    model.eval()  # 모델을 평가 모드로 전환\n",
    "\n",
    "    # 입력 문장 토큰화 (소스 언어로 설정된 상태에서)\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", max_length=max_length, truncation=True, padding=\"max_length\").to(device)\n",
    "\n",
    "    # 소스 문장을 인코딩\n",
    "    with torch.no_grad():\n",
    "        src_mask = torch.zeros((inputs['input_ids'].shape[1], inputs['input_ids'].shape[1]), device=device).type(torch.bool)\n",
    "        memory = model.encode(inputs['input_ids'].transpose(0, 1), src_mask)\n",
    "\n",
    "    # 타겟 문장 시작을 <sos> 토큰으로 설정\n",
    "    tgt_tokens = torch.ones(1, 1).fill_(tokenizer.bos_token_id).type(torch.long).to(device)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt_tokens.size(0)).to(device)\n",
    "\n",
    "        # 디코딩\n",
    "        with torch.no_grad():\n",
    "            output = model.decode(tgt_tokens, memory, tgt_mask)\n",
    "            output = model.generator(output)\n",
    "\n",
    "        #확인용~##############################################################\n",
    "        # Softmax 적용하여 확률로 변환##############################\n",
    "        probabilities = F.softmax(output[-1, :, :], dim=-1)\n",
    "        # 상위 top_k 후보 확인\n",
    "        top_k=5\n",
    "        top_k_probabilities, top_k_indices = torch.topk(probabilities, top_k, dim=-1)\n",
    "\n",
    "        # 출력 후보들을 확인\n",
    "        print(\"\\nTop-k predictions:\")\n",
    "        for i in range(top_k):\n",
    "            token_id = top_k_indices[0, i].item()\n",
    "            token_prob = top_k_probabilities[0, i].item()\n",
    "            token = tokenizer.decode([token_id])\n",
    "            print(f\"Token: {token}, Probability: {token_prob:.4f}\")\n",
    "        ##########################################################\n",
    "\n",
    "        # 가장 가능성 높은 토큰을 선택\n",
    "        next_token = output.argmax(dim=-1)[-1].item()\n",
    "\n",
    "        # <eos> 토큰이 나오면 종료\n",
    "        if next_token == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        # 예측된 토큰을 타겟 토큰 시퀀스에 추가\n",
    "        tgt_tokens = torch.cat([tgt_tokens, torch.tensor([[next_token]], device=device)], dim=0)\n",
    "\n",
    "    # 번역된 토큰을 텍스트로 변환\n",
    "    translated_sentence = tokenizer.decode(tgt_tokens.flatten(), skip_special_tokens=True)\n",
    "\n",
    "    return translated_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0557\n",
      "Token: für, Probability: 0.0445\n",
      "Token: ., Probability: 0.0413\n",
      "Token: ,, Probability: 0.0279\n",
      "Token: er, Probability: 0.0235\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0190\n",
      "Token: rassen, Probability: 0.0175\n",
      "Token: \", Probability: 0.0175\n",
      "Token: ,, Probability: 0.0166\n",
      "Token: trä, Probability: 0.0115\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ,, Probability: 0.0186\n",
      "Token: rassen, Probability: 0.0183\n",
      "Token: S, Probability: 0.0167\n",
      "Token: Person, Probability: 0.0150\n",
      "Token: \", Probability: 0.0130\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.1197\n",
      "Token: als, Probability: 0.1047\n",
      "Token: um, Probability: 0.0856\n",
      "Token: wie, Probability: 0.0712\n",
      "Token: der, Probability: 0.0394\n",
      "\n",
      "Top-k predictions:\n",
      "Token: tur, Probability: 0.0222\n",
      "Token: ,, Probability: 0.0220\n",
      "Token: S, Probability: 0.0185\n",
      "Token: Person, Probability: 0.0182\n",
      "Token: ., Probability: 0.0168\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.1334\n",
      "Token: Person, Probability: 0.0522\n",
      "Token: zh, Probability: 0.0256\n",
      "Token: ,, Probability: 0.0227\n",
      "Token: als, Probability: 0.0174\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ,, Probability: 0.0297\n",
      "Token: tur, Probability: 0.0248\n",
      "Token: ., Probability: 0.0233\n",
      "Token: Person, Probability: 0.0170\n",
      "Token: S, Probability: 0.0158\n",
      "\n",
      "Top-k predictions:\n",
      "Token: als, Probability: 0.0859\n",
      "Token: S, Probability: 0.0825\n",
      "Token: um, Probability: 0.0696\n",
      "Token: wie, Probability: 0.0505\n",
      "Token: , Probability: 0.0301\n",
      "\n",
      "Top-k predictions:\n",
      "Token: „, Probability: 0.0927\n",
      "Token: S, Probability: 0.0710\n",
      "Token: Ele, Probability: 0.0495\n",
      "Token: ein, Probability: 0.0461\n",
      "Token: er, Probability: 0.0429\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0404\n",
      "Token: jack, Probability: 0.0167\n",
      "Token: Person, Probability: 0.0130\n",
      "Token: tte, Probability: 0.0105\n",
      "Token: “., Probability: 0.0089\n",
      "\n",
      "Top-k predictions:\n",
      "Token: \", Probability: 0.0188\n",
      "Token: tur, Probability: 0.0173\n",
      "Token: “, Probability: 0.0152\n",
      "Token: Person, Probability: 0.0152\n",
      "Token: S, Probability: 0.0151\n",
      "\n",
      "Top-k predictions:\n",
      "Token: steht, Probability: 0.0582\n",
      "Token: S, Probability: 0.0419\n",
      "Token: selt, Probability: 0.0256\n",
      "Token: für, Probability: 0.0209\n",
      "Token: “., Probability: 0.0207\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ., Probability: 0.3940\n",
      "Token: ,, Probability: 0.1739\n",
      "Token: für, Probability: 0.0378\n",
      "Token: um, Probability: 0.0362\n",
      "Token: und, Probability: 0.0353\n",
      "\n",
      "Top-k predictions:\n",
      "Token: “, Probability: 0.2397\n",
      "Token: </s>, Probability: 0.0650\n",
      "Token: S, Probability: 0.0268\n",
      "Token: ,, Probability: 0.0194\n",
      "Token: um, Probability: 0.0164\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0750\n",
      "Token: um, Probability: 0.0606\n",
      "Token: ,, Probability: 0.0598\n",
      "Token: als, Probability: 0.0564\n",
      "Token: ., Probability: 0.0398\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ., Probability: 0.0173\n",
      "Token: ,, Probability: 0.0170\n",
      "Token: tur, Probability: 0.0170\n",
      "Token: S, Probability: 0.0170\n",
      "Token: \", Probability: 0.0160\n",
      "\n",
      "Top-k predictions:\n",
      "Token: “, Probability: 0.1878\n",
      "Token: </s>, Probability: 0.0647\n",
      "Token: S, Probability: 0.0266\n",
      "Token: ,, Probability: 0.0257\n",
      "Token: um, Probability: 0.0231\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0712\n",
      "Token: ,, Probability: 0.0639\n",
      "Token: um, Probability: 0.0610\n",
      "Token: als, Probability: 0.0528\n",
      "Token: ., Probability: 0.0374\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ,, Probability: 0.0196\n",
      "Token: ., Probability: 0.0184\n",
      "Token: S, Probability: 0.0167\n",
      "Token: \", Probability: 0.0166\n",
      "Token: tur, Probability: 0.0165\n",
      "\n",
      "Top-k predictions:\n",
      "Token: um, Probability: 0.1943\n",
      "Token: S, Probability: 0.0884\n",
      "Token: als, Probability: 0.0726\n",
      "Token: , Probability: 0.0589\n",
      "Token: der, Probability: 0.0290\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ein, Probability: 0.0462\n",
      "Token: etwas, Probability: 0.0369\n",
      "Token: für, Probability: 0.0259\n",
      "Token: S, Probability: 0.0246\n",
      "Token: “, Probability: 0.0233\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0834\n",
      "Token: „, Probability: 0.0388\n",
      "Token: ., Probability: 0.0364\n",
      "Token: , Probability: 0.0258\n",
      "Token: Mü, Probability: 0.0200\n",
      "\n",
      "Top-k predictions:\n",
      "Token: \", Probability: 0.0191\n",
      "Token: ., Probability: 0.0173\n",
      "Token: ,, Probability: 0.0170\n",
      "Token: tur, Probability: 0.0164\n",
      "Token: S, Probability: 0.0163\n",
      "\n",
      "Top-k predictions:\n",
      "Token: um, Probability: 0.0370\n",
      "Token: steht, Probability: 0.0347\n",
      "Token: für, Probability: 0.0333\n",
      "Token: herum, Probability: 0.0294\n",
      "Token: S, Probability: 0.0294\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ein, Probability: 0.0326\n",
      "Token: etwas, Probability: 0.0312\n",
      "Token: S, Probability: 0.0268\n",
      "Token: für, Probability: 0.0248\n",
      "Token: “, Probability: 0.0233\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0797\n",
      "Token: „, Probability: 0.0390\n",
      "Token: ., Probability: 0.0360\n",
      "Token: , Probability: 0.0286\n",
      "Token: herum, Probability: 0.0214\n",
      "\n",
      "Top-k predictions:\n",
      "Token: \", Probability: 0.0199\n",
      "Token: ., Probability: 0.0171\n",
      "Token: tur, Probability: 0.0164\n",
      "Token: S, Probability: 0.0161\n",
      "Token: ,, Probability: 0.0160\n",
      "\n",
      "Top-k predictions:\n",
      "Token: herum, Probability: 0.0390\n",
      "Token: steht, Probability: 0.0330\n",
      "Token: Aus, Probability: 0.0313\n",
      "Token: um, Probability: 0.0299\n",
      "Token: für, Probability: 0.0296\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ., Probability: 0.2276\n",
      "Token: ,, Probability: 0.0464\n",
      "Token: , Probability: 0.0174\n",
      "Token: für, Probability: 0.0156\n",
      "Token: \", Probability: 0.0126\n",
      "\n",
      "Top-k predictions:\n",
      "Token: “, Probability: 0.1943\n",
      "Token: </s>, Probability: 0.0959\n",
      "Token: ,, Probability: 0.0241\n",
      "Token: S, Probability: 0.0202\n",
      "Token: um, Probability: 0.0163\n",
      "\n",
      "Top-k predictions:\n",
      "Token: herum, Probability: 0.0807\n",
      "Token: ., Probability: 0.0672\n",
      "Token: S, Probability: 0.0662\n",
      "Token: ,, Probability: 0.0660\n",
      "Token: steht, Probability: 0.0403\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ., Probability: 0.2214\n",
      "Token: ,, Probability: 0.0498\n",
      "Token: , Probability: 0.0187\n",
      "Token: für, Probability: 0.0165\n",
      "Token: ist, Probability: 0.0123\n",
      "\n",
      "Top-k predictions:\n",
      "Token: “, Probability: 0.1952\n",
      "Token: </s>, Probability: 0.0795\n",
      "Token: ,, Probability: 0.0223\n",
      "Token: S, Probability: 0.0222\n",
      "Token: e, Probability: 0.0179\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0696\n",
      "Token: ., Probability: 0.0672\n",
      "Token: ,, Probability: 0.0652\n",
      "Token: herum, Probability: 0.0637\n",
      "Token: steht, Probability: 0.0459\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ., Probability: 0.0211\n",
      "Token: \", Probability: 0.0198\n",
      "Token: ,, Probability: 0.0195\n",
      "Token: S, Probability: 0.0179\n",
      "Token: tur, Probability: 0.0158\n",
      "\n",
      "Top-k predictions:\n",
      "Token: “, Probability: 0.1805\n",
      "Token: </s>, Probability: 0.0785\n",
      "Token: ,, Probability: 0.0246\n",
      "Token: S, Probability: 0.0223\n",
      "Token: steht, Probability: 0.0192\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0682\n",
      "Token: ,, Probability: 0.0667\n",
      "Token: ., Probability: 0.0643\n",
      "Token: herum, Probability: 0.0569\n",
      "Token: steht, Probability: 0.0496\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ., Probability: 0.0213\n",
      "Token: ,, Probability: 0.0210\n",
      "Token: \", Probability: 0.0196\n",
      "Token: S, Probability: 0.0178\n",
      "Token: tur, Probability: 0.0154\n",
      "\n",
      "Top-k predictions:\n",
      "Token: “, Probability: 0.1638\n",
      "Token: </s>, Probability: 0.0794\n",
      "Token: ,, Probability: 0.0285\n",
      "Token: S, Probability: 0.0213\n",
      "Token: um, Probability: 0.0203\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0684\n",
      "Token: ,, Probability: 0.0674\n",
      "Token: ., Probability: 0.0611\n",
      "Token: steht, Probability: 0.0532\n",
      "Token: herum, Probability: 0.0515\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ,, Probability: 0.0226\n",
      "Token: ., Probability: 0.0218\n",
      "Token: \", Probability: 0.0193\n",
      "Token: S, Probability: 0.0182\n",
      "Token: um, Probability: 0.0159\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.1067\n",
      "Token: um, Probability: 0.0928\n",
      "Token: als, Probability: 0.0687\n",
      "Token: , Probability: 0.0672\n",
      "Token: der, Probability: 0.0289\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ., Probability: 0.0221\n",
      "Token: ,, Probability: 0.0218\n",
      "Token: \", Probability: 0.0193\n",
      "Token: S, Probability: 0.0180\n",
      "Token: um, Probability: 0.0157\n",
      "\n",
      "Top-k predictions:\n",
      "Token: “, Probability: 0.1512\n",
      "Token: </s>, Probability: 0.0743\n",
      "Token: ,, Probability: 0.0296\n",
      "Token: S, Probability: 0.0220\n",
      "Token: steht, Probability: 0.0219\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0679\n",
      "Token: ,, Probability: 0.0610\n",
      "Token: steht, Probability: 0.0589\n",
      "Token: ., Probability: 0.0559\n",
      "Token: herum, Probability: 0.0470\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ,, Probability: 0.0228\n",
      "Token: ., Probability: 0.0215\n",
      "Token: \", Probability: 0.0190\n",
      "Token: S, Probability: 0.0180\n",
      "Token: um, Probability: 0.0164\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.1027\n",
      "Token: um, Probability: 0.0957\n",
      "Token: als, Probability: 0.0686\n",
      "Token: , Probability: 0.0662\n",
      "Token: der, Probability: 0.0293\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ,, Probability: 0.0222\n",
      "Token: ., Probability: 0.0213\n",
      "Token: \", Probability: 0.0188\n",
      "Token: S, Probability: 0.0183\n",
      "Token: um, Probability: 0.0163\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.1024\n",
      "Token: um, Probability: 0.0963\n",
      "Token: als, Probability: 0.0689\n",
      "Token: , Probability: 0.0650\n",
      "Token: der, Probability: 0.0301\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ,, Probability: 0.0213\n",
      "Token: ., Probability: 0.0212\n",
      "Token: \", Probability: 0.0186\n",
      "Token: S, Probability: 0.0184\n",
      "Token: um, Probability: 0.0164\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.1012\n",
      "Token: um, Probability: 0.0966\n",
      "Token: als, Probability: 0.0697\n",
      "Token: , Probability: 0.0640\n",
      "Token: der, Probability: 0.0305\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ., Probability: 0.0208\n",
      "Token: ,, Probability: 0.0204\n",
      "Token: \", Probability: 0.0186\n",
      "Token: S, Probability: 0.0181\n",
      "Token: um, Probability: 0.0162\n",
      "\n",
      "Top-k predictions:\n",
      "Token: “, Probability: 0.1339\n",
      "Token: </s>, Probability: 0.0684\n",
      "Token: ,, Probability: 0.0294\n",
      "Token: steht, Probability: 0.0248\n",
      "Token: S, Probability: 0.0231\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0702\n",
      "Token: steht, Probability: 0.0695\n",
      "Token: ,, Probability: 0.0490\n",
      "Token: ., Probability: 0.0484\n",
      "Token: herum, Probability: 0.0442\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ,, Probability: 0.0219\n",
      "Token: ., Probability: 0.0212\n",
      "Token: \", Probability: 0.0185\n",
      "Token: S, Probability: 0.0181\n",
      "Token: um, Probability: 0.0169\n",
      "\n",
      "Top-k predictions:\n",
      "Token: um, Probability: 0.0988\n",
      "Token: S, Probability: 0.0970\n",
      "Token: als, Probability: 0.0708\n",
      "Token: , Probability: 0.0647\n",
      "Token: der, Probability: 0.0300\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ein, Probability: 0.0465\n",
      "Token: etwas, Probability: 0.0364\n",
      "Token: „, Probability: 0.0257\n",
      "Token: S, Probability: 0.0243\n",
      "Token: für, Probability: 0.0226\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0915\n",
      "Token: „, Probability: 0.0446\n",
      "Token: , Probability: 0.0372\n",
      "Token: ., Probability: 0.0318\n",
      "Token: Mü, Probability: 0.0234\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ., Probability: 0.0208\n",
      "Token: ,, Probability: 0.0206\n",
      "Token: \", Probability: 0.0191\n",
      "Token: S, Probability: 0.0182\n",
      "Token: um, Probability: 0.0160\n",
      "\n",
      "Top-k predictions:\n",
      "Token: “, Probability: 0.1355\n",
      "Token: </s>, Probability: 0.0709\n",
      "Token: ,, Probability: 0.0296\n",
      "Token: steht, Probability: 0.0243\n",
      "Token: S, Probability: 0.0222\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0691\n",
      "Token: steht, Probability: 0.0655\n",
      "Token: herum, Probability: 0.0527\n",
      "Token: ., Probability: 0.0510\n",
      "Token: ,, Probability: 0.0495\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ,, Probability: 0.0213\n",
      "Token: ., Probability: 0.0207\n",
      "Token: \", Probability: 0.0188\n",
      "Token: S, Probability: 0.0180\n",
      "Token: um, Probability: 0.0163\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.1022\n",
      "Token: um, Probability: 0.0881\n",
      "Token: als, Probability: 0.0693\n",
      "Token: , Probability: 0.0631\n",
      "Token: der, Probability: 0.0304\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ,, Probability: 0.0206\n",
      "Token: ., Probability: 0.0203\n",
      "Token: \", Probability: 0.0188\n",
      "Token: S, Probability: 0.0180\n",
      "Token: be, Probability: 0.0163\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.1021\n",
      "Token: um, Probability: 0.0881\n",
      "Token: als, Probability: 0.0696\n",
      "Token: , Probability: 0.0625\n",
      "Token: der, Probability: 0.0310\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ., Probability: 0.0202\n",
      "Token: ,, Probability: 0.0201\n",
      "Token: \", Probability: 0.0188\n",
      "Token: S, Probability: 0.0179\n",
      "Token: be, Probability: 0.0165\n",
      "\n",
      "Top-k predictions:\n",
      "Token: “, Probability: 0.1337\n",
      "Token: </s>, Probability: 0.0603\n",
      "Token: ,, Probability: 0.0275\n",
      "Token: steht, Probability: 0.0257\n",
      "Token: S, Probability: 0.0245\n",
      "\n",
      "Top-k predictions:\n",
      "Token: steht, Probability: 0.0699\n",
      "Token: S, Probability: 0.0690\n",
      "Token: herum, Probability: 0.0488\n",
      "Token: ., Probability: 0.0474\n",
      "Token: ,, Probability: 0.0459\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ., Probability: 0.3187\n",
      "Token: ,, Probability: 0.1794\n",
      "Token: um, Probability: 0.0482\n",
      "Token: herum, Probability: 0.0288\n",
      "Token: und, Probability: 0.0277\n",
      "\n",
      "Top-k predictions:\n",
      "Token: “, Probability: 0.1317\n",
      "Token: </s>, Probability: 0.0609\n",
      "Token: ,, Probability: 0.0277\n",
      "Token: S, Probability: 0.0247\n",
      "Token: steht, Probability: 0.0238\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0682\n",
      "Token: steht, Probability: 0.0569\n",
      "Token: herum, Probability: 0.0543\n",
      "Token: ., Probability: 0.0506\n",
      "Token: ,, Probability: 0.0493\n",
      "\n",
      "Top-k predictions:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: ,, Probability: 0.0220\n",
      "Token: ., Probability: 0.0218\n",
      "Token: um, Probability: 0.0183\n",
      "Token: S, Probability: 0.0181\n",
      "Token: \", Probability: 0.0178\n",
      "\n",
      "Top-k predictions:\n",
      "Token: um, Probability: 0.1078\n",
      "Token: S, Probability: 0.0959\n",
      "Token: als, Probability: 0.0672\n",
      "Token: , Probability: 0.0662\n",
      "Token: der, Probability: 0.0307\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ein, Probability: 0.0456\n",
      "Token: etwas, Probability: 0.0358\n",
      "Token: „, Probability: 0.0261\n",
      "Token: S, Probability: 0.0239\n",
      "Token: für, Probability: 0.0222\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0893\n",
      "Token: „, Probability: 0.0465\n",
      "Token: , Probability: 0.0369\n",
      "Token: ., Probability: 0.0293\n",
      "Token: Mü, Probability: 0.0243\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ., Probability: 0.0215\n",
      "Token: ,, Probability: 0.0209\n",
      "Token: \", Probability: 0.0189\n",
      "Token: S, Probability: 0.0180\n",
      "Token: um, Probability: 0.0179\n",
      "\n",
      "Top-k predictions:\n",
      "Token: “, Probability: 0.1345\n",
      "Token: </s>, Probability: 0.0623\n",
      "Token: ,, Probability: 0.0268\n",
      "Token: S, Probability: 0.0239\n",
      "Token: steht, Probability: 0.0234\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0680\n",
      "Token: herum, Probability: 0.0624\n",
      "Token: steht, Probability: 0.0566\n",
      "Token: ., Probability: 0.0512\n",
      "Token: ,, Probability: 0.0476\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ,, Probability: 0.0211\n",
      "Token: ., Probability: 0.0211\n",
      "Token: \", Probability: 0.0188\n",
      "Token: um, Probability: 0.0177\n",
      "Token: S, Probability: 0.0174\n",
      "\n",
      "Top-k predictions:\n",
      "Token: um, Probability: 0.1000\n",
      "Token: S, Probability: 0.0972\n",
      "Token: als, Probability: 0.0681\n",
      "Token: , Probability: 0.0646\n",
      "Token: der, Probability: 0.0304\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ein, Probability: 0.0428\n",
      "Token: etwas, Probability: 0.0350\n",
      "Token: „, Probability: 0.0252\n",
      "Token: S, Probability: 0.0232\n",
      "Token: für, Probability: 0.0217\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0863\n",
      "Token: „, Probability: 0.0463\n",
      "Token: , Probability: 0.0375\n",
      "Token: ., Probability: 0.0281\n",
      "Token: Mü, Probability: 0.0244\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ., Probability: 0.0208\n",
      "Token: ,, Probability: 0.0204\n",
      "Token: \", Probability: 0.0192\n",
      "Token: S, Probability: 0.0176\n",
      "Token: um, Probability: 0.0173\n",
      "\n",
      "Top-k predictions:\n",
      "Token: “, Probability: 0.1323\n",
      "Token: </s>, Probability: 0.0634\n",
      "Token: ,, Probability: 0.0264\n",
      "Token: steht, Probability: 0.0235\n",
      "Token: S, Probability: 0.0231\n",
      "\n",
      "Top-k predictions:\n",
      "Token: herum, Probability: 0.0679\n",
      "Token: S, Probability: 0.0664\n",
      "Token: steht, Probability: 0.0565\n",
      "Token: ., Probability: 0.0530\n",
      "Token: ,, Probability: 0.0479\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ., Probability: 0.1648\n",
      "Token: ,, Probability: 0.0455\n",
      "Token: , Probability: 0.0266\n",
      "Token: be, Probability: 0.0192\n",
      "Token: ist, Probability: 0.0138\n",
      "\n",
      "Top-k predictions:\n",
      "Token: “, Probability: 0.1332\n",
      "Token: </s>, Probability: 0.0629\n",
      "Token: ,, Probability: 0.0265\n",
      "Token: steht, Probability: 0.0238\n",
      "Token: S, Probability: 0.0233\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0666\n",
      "Token: herum, Probability: 0.0621\n",
      "Token: steht, Probability: 0.0584\n",
      "Token: ., Probability: 0.0542\n",
      "Token: ,, Probability: 0.0483\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ,, Probability: 0.0216\n",
      "Token: ., Probability: 0.0215\n",
      "Token: \", Probability: 0.0188\n",
      "Token: um, Probability: 0.0182\n",
      "Token: S, Probability: 0.0176\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.1024\n",
      "Token: um, Probability: 0.0930\n",
      "Token: als, Probability: 0.0679\n",
      "Token: , Probability: 0.0637\n",
      "Token: der, Probability: 0.0315\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ,, Probability: 0.0213\n",
      "Token: ., Probability: 0.0212\n",
      "Token: \", Probability: 0.0187\n",
      "Token: um, Probability: 0.0182\n",
      "Token: S, Probability: 0.0178\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.1029\n",
      "Token: um, Probability: 0.0926\n",
      "Token: als, Probability: 0.0681\n",
      "Token: , Probability: 0.0623\n",
      "Token: der, Probability: 0.0315\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ., Probability: 0.0209\n",
      "Token: ,, Probability: 0.0208\n",
      "Token: \", Probability: 0.0189\n",
      "Token: S, Probability: 0.0177\n",
      "Token: um, Probability: 0.0177\n",
      "\n",
      "Top-k predictions:\n",
      "Token: “, Probability: 0.1273\n",
      "Token: </s>, Probability: 0.0617\n",
      "Token: ,, Probability: 0.0267\n",
      "Token: steht, Probability: 0.0250\n",
      "Token: S, Probability: 0.0237\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0672\n",
      "Token: steht, Probability: 0.0625\n",
      "Token: herum, Probability: 0.0593\n",
      "Token: ., Probability: 0.0509\n",
      "Token: ,, Probability: 0.0459\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ,, Probability: 0.0209\n",
      "Token: ., Probability: 0.0207\n",
      "Token: \", Probability: 0.0187\n",
      "Token: S, Probability: 0.0176\n",
      "Token: um, Probability: 0.0176\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.1004\n",
      "Token: um, Probability: 0.0936\n",
      "Token: als, Probability: 0.0683\n",
      "Token: , Probability: 0.0625\n",
      "Token: der, Probability: 0.0309\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ,, Probability: 0.0207\n",
      "Token: ., Probability: 0.0207\n",
      "Token: \", Probability: 0.0187\n",
      "Token: be, Probability: 0.0178\n",
      "Token: um, Probability: 0.0175\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0999\n",
      "Token: um, Probability: 0.0929\n",
      "Token: als, Probability: 0.0690\n",
      "Token: , Probability: 0.0621\n",
      "Token: der, Probability: 0.0312\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ., Probability: 0.0208\n",
      "Token: ,, Probability: 0.0204\n",
      "Token: \", Probability: 0.0187\n",
      "Token: be, Probability: 0.0180\n",
      "Token: um, Probability: 0.0175\n",
      "\n",
      "Top-k predictions:\n",
      "Token: “, Probability: 0.1218\n",
      "Token: </s>, Probability: 0.0594\n",
      "Token: ,, Probability: 0.0269\n",
      "Token: steht, Probability: 0.0258\n",
      "Token: S, Probability: 0.0239\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0665\n",
      "Token: steht, Probability: 0.0650\n",
      "Token: herum, Probability: 0.0567\n",
      "Token: ., Probability: 0.0488\n",
      "Token: ,, Probability: 0.0429\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ., Probability: 0.0207\n",
      "Token: ,, Probability: 0.0204\n",
      "Token: \", Probability: 0.0186\n",
      "Token: um, Probability: 0.0179\n",
      "Token: be, Probability: 0.0178\n",
      "\n",
      "Top-k predictions:\n",
      "Token: “, Probability: 0.1213\n",
      "Token: </s>, Probability: 0.0590\n",
      "Token: ,, Probability: 0.0271\n",
      "Token: steht, Probability: 0.0258\n",
      "Token: S, Probability: 0.0240\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0679\n",
      "Token: steht, Probability: 0.0662\n",
      "Token: herum, Probability: 0.0551\n",
      "Token: ., Probability: 0.0479\n",
      "Token: ,, Probability: 0.0431\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ,, Probability: 0.0213\n",
      "Token: ., Probability: 0.0208\n",
      "Token: be, Probability: 0.0183\n",
      "Token: \", Probability: 0.0183\n",
      "Token: um, Probability: 0.0180\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.1001\n",
      "Token: um, Probability: 0.0946\n",
      "Token: als, Probability: 0.0673\n",
      "Token: , Probability: 0.0637\n",
      "Token: der, Probability: 0.0298\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ,, Probability: 0.0212\n",
      "Token: ., Probability: 0.0211\n",
      "Token: be, Probability: 0.0184\n",
      "Token: \", Probability: 0.0184\n",
      "Token: um, Probability: 0.0177\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0985\n",
      "Token: um, Probability: 0.0946\n",
      "Token: als, Probability: 0.0682\n",
      "Token: , Probability: 0.0629\n",
      "Token: der, Probability: 0.0300\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ., Probability: 0.0213\n",
      "Token: ,, Probability: 0.0209\n",
      "Token: \", Probability: 0.0184\n",
      "Token: be, Probability: 0.0183\n",
      "Token: um, Probability: 0.0178\n",
      "\n",
      "Top-k predictions:\n",
      "Token: “, Probability: 0.1178\n",
      "Token: </s>, Probability: 0.0573\n",
      "Token: ,, Probability: 0.0276\n",
      "Token: steht, Probability: 0.0262\n",
      "Token: S, Probability: 0.0244\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0675\n",
      "Token: steht, Probability: 0.0674\n",
      "Token: herum, Probability: 0.0534\n",
      "Token: ., Probability: 0.0470\n",
      "Token: ,, Probability: 0.0423\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ,, Probability: 0.0215\n",
      "Token: ., Probability: 0.0215\n",
      "Token: be, Probability: 0.0186\n",
      "Token: \", Probability: 0.0182\n",
      "Token: um, Probability: 0.0180\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0979\n",
      "Token: um, Probability: 0.0953\n",
      "Token: als, Probability: 0.0681\n",
      "Token: , Probability: 0.0636\n",
      "Token: der, Probability: 0.0306\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ,, Probability: 0.0215\n",
      "Token: ., Probability: 0.0214\n",
      "Token: be, Probability: 0.0191\n",
      "Token: \", Probability: 0.0183\n",
      "Token: um, Probability: 0.0179\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0974\n",
      "Token: um, Probability: 0.0947\n",
      "Token: als, Probability: 0.0687\n",
      "Token: , Probability: 0.0625\n",
      "Token: der, Probability: 0.0308\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ,, Probability: 0.0212\n",
      "Token: ., Probability: 0.0211\n",
      "Token: be, Probability: 0.0195\n",
      "Token: \", Probability: 0.0183\n",
      "Token: um, Probability: 0.0179\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0957\n",
      "Token: um, Probability: 0.0951\n",
      "Token: als, Probability: 0.0702\n",
      "Token: , Probability: 0.0617\n",
      "Token: der, Probability: 0.0313\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ., Probability: 0.0208\n",
      "Token: ,, Probability: 0.0207\n",
      "Token: be, Probability: 0.0199\n",
      "Token: \", Probability: 0.0182\n",
      "Token: um, Probability: 0.0177\n",
      "\n",
      "Top-k predictions:\n",
      "Token: “, Probability: 0.1144\n",
      "Token: </s>, Probability: 0.0559\n",
      "Token: ,, Probability: 0.0279\n",
      "Token: steht, Probability: 0.0275\n",
      "Token: S, Probability: 0.0243\n",
      "\n",
      "Top-k predictions:\n",
      "Token: steht, Probability: 0.0716\n",
      "Token: S, Probability: 0.0669\n",
      "Token: herum, Probability: 0.0494\n",
      "Token: ., Probability: 0.0450\n",
      "Token: als, Probability: 0.0412\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ., Probability: 0.3174\n",
      "Token: ,, Probability: 0.1775\n",
      "Token: um, Probability: 0.0495\n",
      "Token: herum, Probability: 0.0274\n",
      "Token: ein, Probability: 0.0267\n",
      "\n",
      "Top-k predictions:\n",
      "Token: “, Probability: 0.1145\n",
      "Token: </s>, Probability: 0.0575\n",
      "Token: ,, Probability: 0.0280\n",
      "Token: steht, Probability: 0.0261\n",
      "Token: S, Probability: 0.0244\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0657\n",
      "Token: steht, Probability: 0.0649\n",
      "Token: herum, Probability: 0.0518\n",
      "Token: ., Probability: 0.0463\n",
      "Token: als, Probability: 0.0424\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ., Probability: 0.0216\n",
      "Token: ,, Probability: 0.0215\n",
      "Token: be, Probability: 0.0194\n",
      "Token: um, Probability: 0.0188\n",
      "Token: \", Probability: 0.0178\n",
      "\n",
      "Top-k predictions:\n",
      "Token: “, Probability: 0.1133\n",
      "Token: </s>, Probability: 0.0568\n",
      "Token: ,, Probability: 0.0284\n",
      "Token: steht, Probability: 0.0267\n",
      "Token: S, Probability: 0.0247\n",
      "\n",
      "Top-k predictions:\n",
      "Token: S, Probability: 0.0670\n",
      "Token: steht, Probability: 0.0661\n",
      "Token: herum, Probability: 0.0496\n",
      "Token: ., Probability: 0.0448\n",
      "Token: als, Probability: 0.0420\n",
      "\n",
      "Top-k predictions:\n",
      "Token: ,, Probability: 0.0215\n",
      "Token: ., Probability: 0.0215\n",
      "Token: be, Probability: 0.0194\n",
      "Token: um, Probability: 0.0188\n",
      "Token: \", Probability: 0.0177\n",
      "\n",
      " sentence : Have a good day.\n",
      "translated_sentence : S S, Stur S, als „ S\" steht.“ S.“ S, um ein S\" um ein S\" herum.“ herum.“ S.“ S.“ S, S.“ S, S, S, S.“ S, um ein S.“ S, S, S.“ steht.“ S, um ein S.“ S, um ein S.“ herum.“ S, S, S.“ S, S, S.“ S.“ S, S, S.“ S, S, S, S.“ steht.“ S.“ S,\n"
     ]
    }
   ],
   "source": [
    "sentence='Have a good day.'\n",
    "translated_sentence = translate(model, sentence, tokenizer, DEVICE)\n",
    "print(f'\\n sentence : {sentence}')\n",
    "print(f'translated_sentence : {translated_sentence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoderLayer, TransformerDecoderLayer\n",
    "\n",
    "class ParallelTransformer(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, forward_expansion, num_layers, src_vocab_size, tgt_vocab_size, max_length, device):\n",
    "        super(ParallelTransformer, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # 인코더와 디코더 임베딩\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, embed_size)\n",
    "        \n",
    "        # 포지셔널 인코딩\n",
    "        self.positional_encoding = PositionalEncoding(embed_size, max_length)\n",
    "\n",
    "        # 병렬 인코더-디코더 레이어 (모든 레이어가 병렬로 실행됨)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=forward_expansion * embed_size)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(d_model=embed_size, nhead=num_heads, dim_feedforward=forward_expansion * embed_size)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # 출력 레이어\n",
    "        self.fc_out = nn.Linear(embed_size, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        # 임베딩과 포지셔널 인코딩 적용\n",
    "        src = self.src_embedding(src) + self.positional_encoding(src)\n",
    "        tgt = self.tgt_embedding(tgt) + self.positional_encoding(tgt)\n",
    "\n",
    "        # 병렬 인코더-디코더 실행\n",
    "        encoder_outputs = []\n",
    "        decoder_outputs = []\n",
    "        for encoder_layer, decoder_layer in zip(self.encoder_layers, self.decoder_layers):\n",
    "            enc_output = encoder_layer(src, src_mask)  # 각 인코더 레이어 병렬 실행\n",
    "            encoder_outputs.append(enc_output)\n",
    "\n",
    "            dec_output = decoder_layer(tgt, enc_output, tgt_mask, src_mask)  # 각 디코더 레이어 병렬 실행\n",
    "            decoder_outputs.append(dec_output)\n",
    "\n",
    "        # 각 레이어의 출력값을 평균하여 최종 출력으로 사용\n",
    "        encoder_final_output = torch.stack(encoder_outputs, dim=0).mean(dim=0)\n",
    "        decoder_final_output = torch.stack(decoder_outputs, dim=0).mean(dim=0)\n",
    "\n",
    "        # 출력층 적용\n",
    "        output = self.fc_out(decoder_final_output)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.encoding = torch.zeros(max_length, embed_size)\n",
    "        self.encoding.requires_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_length).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-torch.log(torch.tensor(10000.0)) / embed_size))\n",
    "\n",
    "        self.encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        self.encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        return self.encoding[:seq_len, :].unsqueeze(0).repeat(batch_size, 1, 1).to(x.device)\n",
    "\n",
    "# 모델 하이퍼파라미터 설정\n",
    "embed_size = 512\n",
    "num_heads = 4\n",
    "forward_expansion = 4\n",
    "num_layers = 4\n",
    "src_vocab_size = tokenizer.vocab_size\n",
    "tgt_vocab_size = tokenizer.vocab_size\n",
    "max_length = 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 모델 초기화\n",
    "model = ParallelTransformer(embed_size, num_heads, forward_expansion, num_layers, src_vocab_size, tgt_vocab_size, max_length, device).to(device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
